{"name":"Fullyconnecteddeepneuralnetwork","tagline":"Fully Connected Modularized Neural Network, can be easily extended to be Deep Network with little extension code","body":"# FullyConnectedDeepNeuralNetwork\r\n\r\nAbout this code\r\n===\r\nThis is one modularized version of fully connected neural network\r\nIn the initial setting, you can specify input dimensions, number of layers and\r\nalso number of node in each layers. The activation function is also flexible,\r\nwhich allows you to change between ReLU and Sigmoid. If you are interested in\r\nusing other activation function, please add it in src/network/activations.py\r\n\r\nSome implementations of ANN online have similar structure with this one. However,\r\nbe aware that the inner implementation are slightly different. My purpose of this\r\nimplementation is to show the clear forward and backward structure of Neural\r\nnetwork, but not for high speed running.\r\n\r\nThis is not GPU version! Do not expect this one can run fast...\r\n\r\nPlease DO NOT copy this code for any kind of assignment that released by your\r\ncourse lecturer! \r\n\r\nHow to train deep network?\r\n===\r\nYou need to generate network with single hidden layer, and set the output of the \r\nnetwork with inputs. After converging, store the weight matrix from input to\r\nhidden layer and discard others. Then one layer by one layer, you will got all the\r\nweights of the deep network. Then, its time to generate the deep network and feed\r\ntrained weights into it. Make sure to train the network as well! It at least can\r\noptimize three layers close to output!!\r\n\r\nSettings\r\n===\r\nNetwork initial settings\r\n```python\r\ninitial_setting = {\r\n    \"inputs_N\"          : 2,       #input dimensions\r\n    \"layers\"            : [ (20, network.ReLU),(1, network.Sigmoid)], #(number of node, type of nonlinear function)\r\n    \"weights_L\"         : -0.1, #Random weight initialization bound    \r\n    \"weights_H\"         : 0.1, #Random weight initialization bound      \r\n    \"save\"              : False, #haven't implement this one yet  \r\n    \"loss\"              : network.CrossEntropyLoss #Cost Function you want to use Cross-Entropy or Mean Squared Error \r\n                   }\r\n```\r\n\r\nYou can also change some of the settings later(after seeing data), then you need the following code:\r\n```python\r\nreplace_value_with_definition(\"inputs_N\",n)\r\nreplace_value_with_definition(\"weights_L\",-1.0/n)\r\nreplace_value_with_definition(\"weights_H\",1.0/n)\r\n```\r\n\r\n## Cross-Entropy Objective vs Epoch Plot\r\n\r\nBlue Line is training data curve, green line is testing data curve.(please ignore the title..)\r\n\r\n\r\n```python\r\nfrom IPython.display import Image\r\nImage(filename='objective.png') \r\n```\r\n\r\n\r\n\r\n\r\n![curve](https://github.com/wuga214/FullyConnectedDeepNeuralNetwork/blob/master/dnn_curve.png)\r\n\r\n\r\n\r\n## Error vs Epoch\r\n\r\nBlue Line is training data curve, green line is testing data curve.(please ignore the title..)\r\n\r\n\r\n```python\r\nfrom IPython.display import Image\r\nImage(filename='error.png') \r\n```\r\n\r\n\r\n\r\n\r\n![error](https://github.com/wuga214/FullyConnectedDeepNeuralNetwork/blob/master/error.png)\r\n\r\n\r\n\r\n## Test Error vs Mini Batch Size\r\n\r\nThough size of mini batch doesn't effect the final result too much, the larger mini batch will cause the curve shaking.\r\n\r\n\r\n```python\r\nfrom IPython.display import Image\r\nImage(filename='batchsize.png') \r\n```\r\n\r\n\r\n\r\n\r\n![batchsize](https://github.com/wuga214/FullyConnectedDeepNeuralNetwork/blob/master/batchsize.png)\r\n\r\n\r\n\r\n## Test Error vs Learning Rate\r\n\r\nLarger learning rate can cause the curve shake, whereas smaller learning rate cause error decrease slow. The following plot shows such property. We then selecte 10e-5 as our learning rate.\r\n\r\n\r\n```python\r\nfrom IPython.display import Image\r\nImage(filename='learningrate.png')\r\n```\r\n\r\n\r\n\r\n\r\n![learningrate](https://github.com/wuga214/FullyConnectedDeepNeuralNetwork/blob/master/learningrate.png)\r\n\r\n\r\n\r\n## Test Error vs Number of Hidden Nodes\r\n\r\nIf number of the hidden nodes is not enough, the complexity of model is limited. The network cannot optimize the objective due to the model complexity limitation. However, too large number of hidden nodes doesn't help to further improve the performance because it is already enough to represent target function. In the following plot, we show that 100 hidden nodes is enough for the task.\r\n\r\n\r\n```python\r\nfrom IPython.display import Image\r\nImage(filename='numberofhidden.png')\r\n```\r\n\r\n\r\n\r\n\r\n![numberofhidden](https://github.com/wuga214/FullyConnectedDeepNeuralNetwork/blob/master/numberofhidden.png)\r\n\r\nData\r\n===\r\n![alt tag](https://github.com/wuga214/FullyConnectedDeepNeuralNetwork/blob/master/DATA.png)\r\n[CIFAR Website](https://www.cs.toronto.edu/~kriz/cifar.html)\r\n\r\nUpdate\r\n===\r\n1. Now momentum is fully activated![2016-02-09]\r\n2. Visualization for learning curve is added[2016-02-12] \r\n3. Experiment functions is built![2016-02-14]\r\n\r\nIssue\r\n===\r\n1. Haven't implement saving function. So the trained model will lose after finishing the program..\r\n2. Doesn't include random dropout mechanism. So you need to trace testing error to control training epoch..\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}